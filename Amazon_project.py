# -*- coding: utf-8 -*-
"""Another_copy_of_amazon_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17sBDyPMon5VfEiFv1ng-WE0ltNulSa_v

Data Exploration

In our initial exploration, we will load the data sets and see what data attributes are available to us. We will also plot the variables, to see if we can find some trends in the data, and explore the possibility of engineering some additional features. We will do the data loading and analysis in pandas, so let us load that library and begin exploring.
"""

import pandas as pd

from google.colab import files

uploaded = files.upload()

#load the training set
df_train = pd.read_csv("AMZN_train.csv")

# (#rows, #columns)
df_train.shape

"""Pandas' info() method is really useful to get a quick peek at the column names, types, non-null counts, as well as the data set's memory size! We have mostly numerical columns (5 floats, 1 int) and an object column - the date. No null values at all, and just 261 KB in size."""

df_train.info()

"""The training data set consists of 4781 rows and 7 columns. These are presented above. In the cell below, we use the describe() method to get a quick overview of our data set. By default, this includes only numerical columns, since the summary statistics are different for attributes of different types (for example, the date, or other categorical attributes). For numerical columns, this method outputs the non-null count, the mean, standard deviation, the minimum and maximum value, and the 25th, 50th and 75th percentile value."""

df_train.describe()

"""If we try to run the same method on the Date column (which is treated as an object) we get the following output: a count of non-null values, a count of unique values, the top (or max) value and its corresponding frequency. While this does give us some information, such that there is exactly one row for each day, and that there are no days with a missing date, it is missing the time range for the column, so what we can do, is try to run min() and max() to get the first and last date for the data set. These are 1997-05-15 and 2016-05-13, respectively.

"""

# the above call to describe() works only for numerical columns
# 'Date' is an object and we need to call it separately
df_train["Date"].describe()

# print the time range
df_train["Date"].min(), df_train["Date"].max()

"""One last thing we can try in the initial analysis of the data is to plot the stock prices as a function of time. These plots are the first thing you see on any stock market. We can use the pandas' plot(...) method to plot lines, bypassing the Date column as an argument for the x axis, and the closing, opening, highest and lowest price to be plotted on the y axis. For readability, we also set the figure size to (12, 9).

"""

from matplotlib import pyplot as plt

plt.rcParams["figure.figsize"] = (12, 9)

_ = df_train.plot(x="Date", y=["Close", "Open", "High", "Low"])

"""All prices are very close to each other since the lines are overlapping nearly everywhere. But, the price has grown significantly over the years, starting at around $100 at the beginning of the century, rising to almost \$700 only 13 years later!

We would like to repeat the same analysis for the validation and testing set, to make sure that they follow a similar distribution and that there are no surprising errors there. It makes sense to create a function to repeat the process we did for the training set, instead of having to rewrite (or copy-paste) the same code again. Let's do that.

"""

def analyse(dataframe):
    """Runs an exploration analysis of the dataframe."""
    print("Shape", dataframe.shape, "\n")
    print("Columns", dataframe.columns, "\n")
    dataframe.info()
    print("\n", dataframe.describe(), "\n")
    print("The data ranges from", dataframe["Date"].min(), "to", dataframe["Date"].max())
    dataframe.plot(x="Date", y=["Close", "Open", "High", "Low"])

# read validation and test sets and then analyse them
df_val = pd.read_csv("AMZN_val.csv")
analyse(df_val)

"""The validation set has the same properties as the training set: no null values, clear column types, and a valid date range. The stocks have continued to grow in this period."""

df_test = pd.read_csv("AMZN_test.csv")
analyse(df_test)

"""The same comment that holds for the validation set, is also true for the test set. The stocks again continue to grow, but we observe some volatility in this period.

Once we have done our exploration of the data, we can move on to the predictive modeling part of the task. The task was to predict if the next day's closing price will be higher than the opening price. We do not have that information explicitly in our data, so we have to infer it.

This is relatively simple, we just need to compare the closing and opening prices one day in advance.

To achieve that, first, we will make sure that the data is sorted by the date. We can use the sort_values method and pass in the Date column as a parameter, sorting it in ascending order.

Next, we need to shift the DataFrame by one row / one day and compare the prices. Pandas has a method for doing exactly that, the shift method. We specify a period of minus one (so that we shift the data from the next day back). Because it is a logical operation, Pandas would return a True / False result for each comparison. We want this to be presented as 1 / 0 for the machine learning models, so we will map it to type int. To store all of this information, we will create a new column, called Target.

"""

# make sure that our data is sorted by date
df_train.sort_values(by="Date", inplace=True)
df_val.sort_values(by="Date", inplace=True)
df_test.sort_values(by="Date", inplace=True)

# notice that we shift by a period of '-1', this takes the next day's price direction for the current day
# a positive period will take the days from the past
df_train["Target"] = (df_train["Close"] > df_train["Open"]).shift(periods=-1, fill_value=0).astype(int)

df_train

"""Running the value_counts method on the Target column gives us the distribution. We have 2292 days where the closing price is higher than the opening and 2389 where it is lower."""

df_train["Target"].value_counts()

"""We are going to repeat the same procedure for the validation as well as the test set."""

df_val["Target"] = (df_val["Close"] > df_val["Open"]).shift(periods=-1, fill_value=0).astype(int)
df_val["Target"].value_counts()

df_test["Target"] = (df_test["Close"] > df_test["Open"]).shift(periods=-1, fill_value=0).astype(int)
df_test["Target"].value_counts()

"""At this point, we can start building some machine learning models to predict the target variable. But, before we do that, it might be useful to engineer some additional features that may help us better predict the price direction. In the next section, we will explore some possibilities for doing feature engineering.

**Feature** Engineering



We know that the stock prices are time-dependent and that the next day's price depends on prices (and many other things) from previous days.

We want to somehow take into account all the values in the last n days, capturing the trend, or the magnitude of price change.

A simple solution would be to calculate a moving average. A moving average computes the arithmetic mean of a sliding window. For each day X, the moving average of order n would be the arithmetic mean of the prices from the days X - 1, X - 2, ... X - n. Pandas has implemented this method. The rolling method provides us with an interface for sliding (in Pandas terminology - rolling) window calculations. The following cells calculate the 3- and 7-days moving average, and add them as a feature into the data set.

Remember that we have our data sorted from before. If the data is not sorted by the date, the results from the rolling() method would be invalid.
"""

df_train["Moving_Average_3"] = (df_train["Close"] - df_train["Open"]).rolling(window=3, min_periods=1).mean()
df_val["Moving_Average_3"] = (df_val["Close"] - df_val["Open"]).rolling(window=3, min_periods=1).mean()
df_test["Moving_Average_3"] = (df_test["Close"] - df_test["Open"]).rolling(window=3, min_periods=1).mean()

df_train["Moving_Average_7"] = (df_train["Close"] - df_train["Open"]).rolling(window=7, min_periods=1).mean()
df_val["Moving_Average_7"] = (df_val["Close"] - df_val["Open"]).rolling(window=7, min_periods=1).mean()
df_test["Moving_Average_7"] = (df_test["Close"] - df_test["Open"]).rolling(window=7, min_periods=1).mean()

"""Feature engineering can be more simpler, we can just take the current day's price direction, or the price range. Anything that comes to mind, that can be considered to be useful, should be tested and tried. Feel free to experiment with other features that you can come up on your own.

"""

# current price direction
df_train["Today_Direction"] = df_train["Close"] - df_train["Open"]
df_val["Today_Direction"] = df_val["Close"] - df_val["Open"]
df_test["Today_Direction"] = df_test["Close"] - df_test["Open"]

# price range
df_train["Price_Range"] = df_train["High"] - df_train["Low"]
df_val["Price_Range"] = df_val["High"] - df_val["Low"]
df_test["Price_Range"] = df_test["High"] - df_test["Low"]

"""The next cell displays a sample of the data with the new features included."""

df_train.sample(10, random_state=42)

"""
Classical Machine Learning Algorithms

The sklearn library is the most popular library in Python for implementing classical machine learning algorithms. We can use it to try and test a few of them. In the following cells we implement:
Logistic Regression
Decision Tree
Random Forest
Gradiaent boosting ensemble

All of these algorithms are implemented in the sklearn.linear_model module. All of them share also the same interface, i.e., we can use the same procedure for implementing any of them.

After fitting them to the training data, we are going to evaluate their performance on the validation set by estimating the AUC metric.

For easier manipulation, we will specify the target column and the feature (input) columns as lists, which we will then use to project the corresponding data frame.

"""

# this is the target column that we aim to predict
y_col = "Target"
# these are the input features for the models
X_cols = [
    "Open",
    "Close",
    "High",
    "Low",
    "Volume",
    "Adj Close",
    "Today_Direction",
    "Price_Range",
    "Moving_Average_3",
    "Moving_Average_7"
]

X_train = df_train[X_cols]
y_train = df_train[y_col]

X_val = df_val[X_cols]
y_val = df_val[y_col]

X_test = df_val[X_cols]
y_test = df_val[y_col]

"""
Logistic Regression

We start our modeling phase with a LogisticRegression model. The inner workings of this algorithm are very similar to that of linear regression, with the difference being that we use logistic regression for classification by modifying the output with a sigmoid function.

To calculate the AUC value, and to plot the ROC curve, we are using the plot_roc_curve(...) method from the sklearn.metrics module and we pass in the fitted model and the validation set.

For this, and throughout the latter algorithms, we are going to use the default parameters for the models. Feel free to explore the documentation and experiment with different values, and try to get a better AUC. However, for reproducibility, i.e., control over the random parts of the algorithm, we are going to set a fixed random seed.
"""

!pip install --upgrade scikit-learn

# for reproducibility
RANDOM_SEED = 42

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have already split your data into training and validation sets (X_train, X_val, y_train, y_val)

# use default parameters
lr = LogisticRegression()

# fit to train set
lr.fit(X_train, y_train)

# predict probabilities on the validation set
y_prob = lr.predict_proba(X_val)[:, 1]

# calculate ROC curve
fpr, tpr, _ = roc_curve(y_val, y_prob)

# calculate AUC
roc_auc = auc(fpr, tpr)

# plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""Logistic regression does not work well for this problem. Having AUC < 0.5 means that the classifier is worse than just randomly guessing the output. Given that we are dealing with a very difficult problem, any AUC > 0.5 would suffice for this task."""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Assuming predictions for the test set (y_pred_lr_test)
y_pred_lr_test = lr.predict(X_test)

# Calculate and print accuracy
accuracy_lr = accuracy_score(y_test, y_pred_lr_test)
print(f'Logistic Regression Accuracy: {accuracy_lr:.4f}')

# Calculate and print precision
precision_lr = precision_score(y_test, y_pred_lr_test)
print(f'Logistic Regression Precision: {precision_lr:.4f}')

# Calculate and print recall
recall_lr = recall_score(y_test, y_pred_lr_test)
print(f'Logistic Regression Recall: {recall_lr:.4f}')

# Calculate and print F1 score
f1_lr = f1_score(y_test, y_pred_lr_test)
print(f'Logistic Regression F1 Score: {f1_lr:.4f}')

# Confusion matrix
conf_matrix_lr = confusion_matrix(y_test, y_pred_lr_test)
print('Logistic Regression Confusion Matrix:')
print(conf_matrix_lr)

"""**Decision Tree**

> Indented block


Next, we will try a decision tree. Decision trees deal better with non-linear spaces, so they might be able to produce a better model than logistic regression.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


# use random seed
RANDOM_SEED = 42

# instantiate the Decision Tree Classifier
dt = DecisionTreeClassifier(random_state=RANDOM_SEED)

# fit to train set
dt.fit(X_train, y_train)

# predict probabilities on the validation set
y_prob_dt = dt.predict_proba(X_val)[:, 1]

# calculate ROC curve
fpr_dt, tpr_dt, _ = roc_curve(y_val, y_prob_dt)

# calculate AUC
roc_auc_dt = auc(fpr_dt, tpr_dt)

# plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_dt, tpr_dt, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc_dt))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Decision Tree')
plt.legend(loc='lower right')

"""The decision tree outperforms the logistic regression model by 0.02, and its AUC is above 0.5!"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Assuming predictions for the test set (y_pred_dt_test)
y_pred_dt_test = dt.predict(X_test)

# Calculate and print accuracy
accuracy_dt = accuracy_score(y_test, y_pred_dt_test)
print(f'Decision Tree Accuracy: {accuracy_dt:.4f}')

# Calculate and print precision
precision_dt = precision_score(y_test, y_pred_dt_test)
print(f'Decision Tree Precision: {precision_dt:.4f}')

# Calculate and print recall
recall_dt = recall_score(y_test, y_pred_dt_test)
print(f'Decision Tree Recall: {recall_dt:.4f}')

# Calculate and print F1 score
f1_dt = f1_score(y_test, y_pred_dt_test)
print(f'Decision Tree F1 Score: {f1_dt:.4f}')

# Confusion matrix
conf_matrix_dt = confusion_matrix(y_test, y_pred_dt_test)
print('Decision Tree Confusion Matrix:')
print(conf_matrix_dt)

"""**Random Forest**

> Indented block

Now we will try to use many decision trees, i.e., a forest. Random forest is an ensemble model that builds multiple decision trees, each with a different (random) sub-set of attributes. It is generally expected that an ensemble model would outperform a base learner (i.e., the combination of predictions from many decision trees would be better / more stable than the prediction of just one decision tree).
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


# use random seed
RANDOM_SEED = 42

# instantiate the Random Forest Classifier
rf = RandomForestClassifier(random_state=RANDOM_SEED)

# fit to train set
rf.fit(X_train, y_train)

# predict probabilities on the validation set
y_prob_rf = rf.predict_proba(X_val)[:, 1]

# calculate ROC curve
fpr_rf, tpr_rf, _ = roc_curve(y_val, y_prob_rf)

# calculate AUC
roc_auc_rf = auc(fpr_rf, tpr_rf)

# plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc_rf))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest')
plt.legend(loc='lower right')
plt.show()

"""Contrary to our expectations, the model does not outperform the decision tree, it actually performs the same."""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Assuming predictions for the test set (y_pred_rf_test)
y_pred_rf_test = rf.predict(X_test)

# Calculate and print accuracy
accuracy_rf = accuracy_score(y_test, y_pred_rf_test)
print(f'Random Forest Accuracy: {accuracy_rf:.4f}')

# Calculate and print precision
precision_rf = precision_score(y_test, y_pred_rf_test)
print(f'Random Forest Precision: {precision_rf:.4f}')

# Calculate and print recall
recall_rf = recall_score(y_test, y_pred_rf_test)
print(f'Random Forest Recall: {recall_rf:.4f}')

# Calculate and print F1 score
f1_rf = f1_score(y_test, y_pred_rf_test)
print(f'Random Forest F1 Score: {f1_rf:.4f}')

# Confusion matrix
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf_test)
print('Random Forest Confusion Matrix:')
print(conf_matrix_rf)

"""Gradient Boosting Ensemble
**bold text**

> Indented block

One last ensemble technique that we would like to try is gradient boosting. A gradient boosting classifier is structurally the same as any ensemble learner - it is a collection of base learners. The algorithm induces (learns) the trees one by one, and in each iteration, it gives greater weight to those instances that were wrongly misclassified, so the next tree would "pay more attention" while training on them. Sometimes this can improve performance.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# use random seed
RANDOM_SEED = 42

# instantiate the Gradient Boosting Classifier
gb = GradientBoostingClassifier(random_state=RANDOM_SEED)

# fit to train set
gb.fit(X_train, y_train)

# predict probabilities on the validation set
y_prob_gb = gb.predict_proba(X_val)[:, 1]

# calculate ROC curve
fpr_gb, tpr_gb, _ = roc_curve(y_val, y_prob_gb)

# calculate AUC
roc_auc_gb = auc(fpr_gb, tpr_gb)

# plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_gb, tpr_gb, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc_gb))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Gradient Boosting')
plt.legend(loc='lower right')
plt.show()

"""Indeed it does, the gradient boosting classifier outperform previous models and scored 0.55 AUC!

We find out that gradient boosting works best for this data set.

"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Assuming predictions for the test set (y_pred_gb_test)
y_pred_gb_test = gb.predict(X_test)

# Calculate and print accuracy
accuracy_gb = accuracy_score(y_test, y_pred_gb_test)
print(f'Gradient Boosting Accuracy: {accuracy_gb:.4f}')

# Calculate and print precision
precision_gb = precision_score(y_test, y_pred_gb_test)
print(f'Gradient Boosting Precision: {precision_gb:.4f}')

# Calculate and print recall
recall_gb = recall_score(y_test, y_pred_gb_test)
print(f'Gradient Boosting Recall: {recall_gb:.4f}')

# Calculate and print F1 score
f1_gb = f1_score(y_test, y_pred_gb_test)
print(f'Gradient Boosting F1 Score: {f1_gb:.4f}')

# Confusion matrix
conf_matrix_gb = confusion_matrix(y_test, y_pred_gb_test)
print('Gradient Boosting Confusion Matrix:')
print(conf_matrix_gb)

"""**Conclusion**


The gradient boosting classifier provided the best AUC score on the validation set. It is a common machine learning practice to train multiple models on the same train/validation data set and provide a model that works best. To simulate a production environment, we have held the test set aside until now.

In the next cell, we are going to evaluate the performance of the gradient boosting classifier on the test set. This is simple as calling plot_roc_curve with the test set instead of the validation one.

In addition, we are showing a feature importance plot, which plots the importance of each feature in regard to the predictive performance of the model (the higher the value the more important the feature is for determining the value of the target variable).

"""

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have already trained the Gradient Boosting Classifier (gb) on the training set
# and you have the test set (X_test, y_test) available

# predict probabilities on the test set
y_prob_gb_test = gb.predict_proba(X_test)[:, 1]

# calculate ROC curve
fpr_gb_test, tpr_gb_test, _ = roc_curve(y_test, y_prob_gb_test)

# calculate AUC
roc_auc_gb_test = auc(fpr_gb_test, tpr_gb_test)

# plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_gb_test, tpr_gb_test, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc_gb_test))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Gradient Boosting (Test Set)')
plt.legend(loc='lower right')
plt.show()

import numpy as np
# Calculate feature importances
importances = gb.feature_importances_
# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Rearrange feature names so they match the sorted feature importances
names = [df_train[X_cols].columns[i] for i in indices]

_ = plt.figure(figsize=(9, 7))
plt.bar(names, importances[indices])
_ = plt.title("Feature importance")
_ = plt.xticks(rotation=20, fontsize = 8)
